---
 title: "MCMC diagnostics - Growth"
 subtitle: "Test v10.0 - Von Bertalanffy model"
 author: "Will Vieira"
 date: "`r paste('Last updated on', format(Sys.time(), '%d %B, %Y'))`"
---


## Diagnostics for the Von Bertalanffy model

```{r, echo = F, message=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(data.table))
suppressPackageStartupMessages(library(cmdstanr))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(purrr))
suppressPackageStartupMessages(library(ggpubr))
suppressPackageStartupMessages(library(tidybayes))
suppressPackageStartupMessages(library(bayesplot))
suppressPackageStartupMessages(library(ggridges))
suppressPackageStartupMessages(require(RColorBrewer))
suppressPackageStartupMessages(library(DT))
```


```{r load simulations, echo = F}
# Load simulation variables
simInfo <- yaml::read_yaml('_simulation_info.yml')

spIds <- simInfo$spIds
simName <- simInfo$simName
simulations <- simInfo$simulations

# List output files and set vector names to species_id
posterior_files <- grep(
  'posterior',
  dir(file.path('output', simName), full.names = TRUE),
  value = TRUE
)
names(posterior_files) <- names(sort(sapply(spIds, function(x) grep(x, posterior_files))))

diag_files <- grep(
  'diagnostics',
  dir(file.path('output', simName), full.names = TRUE),
  value = TRUE
)
names(diag_files) <- names(sort(sapply(spIds, function(x) grep(x, diag_files))))

trainData_files <- grep(
  'trainData',
  dir(file.path('output', simName), full.names = TRUE),
  value = TRUE
)
names(trainData_files) <- names(sort(sapply(spIds, function(x) grep(x, trainData_files))))
```



# Rhat

```{r rhat, echo = F, fig.height = 7, fig.width = 7}
map2_dfr(
  diag_files,
  names(diag_files),
  ~ readRDS(.x)[['rhat']] |>
      bind_cols(species_id = .y)
) |>
ggplot(aes(x = rhat, y = species_id)) +
  geom_boxplot()
```



# Divergent transitions

```{r divergentTransitions, echo = FALSE}
map2_dfr(
  diag_files,
  names(diag_files),
  ~ readRDS(.x)[['diag_summary']]['num_divergent'] |>
      bind_cols(species_id = .y) |>
      mutate(chain = row_number())
) |>
pivot_wider(
  names_from = chain,
  values_from = num_divergent,
  names_prefix = 'chain '
) |>
DT::datatable()
```



# Posterior distribution of parameters

```{r posteriorDist, eval = TRUE,echo = F, fig.height = 7, fig.width = 8}
post <- map2_dfr(
  posterior_files,
  names(posterior_files),
  ~ readRDS(.x) |>
    bind_cols(species_id = .y)
)

for(Par in unique(post$par))
{
  print(
    post |>
      filter(par == Par) |>
      ggplot(aes(x = value, y = species_id)) +
        stat_eye() +
        ggtitle(Par) +
        xlab('')
  )
}
```


# Correlation between parameters

```{r parCorrelation, eval = TRUE,echo = F, fig.height = 8, fig.width = 8}

parComb <- post |>
  summarise(uqPar = unique(par)) |>
  pull(uqPar) |>
  combn(2)

for(i in 1:ncol(parComb))
{
  print(
    post |>
      filter(par %in% parComb[, i]) |>
      filter(iter %in% sample(iter, 1000)) |>
      select(par, value, iter, species_id) |>
      pivot_wider(names_from = par, values_from = value) |>
      ggplot(aes_string(x = parComb[1, i], y = parComb[2, i])) + 
          geom_point() +
          facet_wrap(~ species_id) +
          xlab(parComb[1, i]) +
          ylab(parComb[2, i])
  )
}
```

# Data distribution

```{r organize data, echo = FALSE, warning=FALSE, message=FALSE, fig.height = 7, fig.width = 8}
trainData <- map2_dfr(
  trainData_files,
  names(trainData_files),
  ~ readRDS(.x)
)

treeData <- readRDS('data/treeData.RDS')
treeData <- treeData[species_id %in% spIds]

# merge with train data
treeData[
  trainData,
  sampled := i.sampled,
  on = c('tree_id', 'year_measured')
]

# split between train and everything else (merge validate with rest of data)
treeData[, sampled2 := sampled]
treeData[is.na(sampled), sampled2 := 'validation']

treeData[,
  time := year_measured - year_measured[which.min(year_measured)],
  by = tree_id
]

treeData[, .(species_id, dbh, time, sampled2)] |>
  mutate(sample = sampled2) |>
  pivot_longer(
    cols = c(dbh, time),
    names_to = 'var'
  ) |>
  ggplot(aes(x = value, y = species_id, color = sample, fill = sample)) +
    geom_density_ridges(alpha = 0.5) +
    facet_wrap(~var, scales = 'free_x') +
    xlab('')
```



# Predictions

```{r,echo=FALSE,fig.height = 6.5, fig.width = 8}
post |>
  filter(par %in% c('r', 'Lmax')) |>
  pivot_wider(
    names_from = par,
    values_from = value
  ) |>
  group_by(species_id) |>
  expand_grid(time = seq(0, 2000, 100)) |>
  mutate(Lt = Lmax * (1 - exp(-r * time))) |>
  ggplot(aes(x = time, y = Lt, color = species_id)) +
        stat_lineribbon(.width = 0.95, alpha = .8) +
        scale_fill_brewer() +
        #theme(legend.position="none") +
        ylab('DBH size (mmm)') + xlab('Time (years)')
```

# Out-of-bag cross-validation

To assess model generality we test how good the model is in predicting new observations. The following figures show a 1:1 plot of observed vs predicted data for two diferent measures: DBH size (directly predicted by the model) and annual growth rate (derived from predicted size). We also plot the distribution of the observed vs the predicted measures.

```{r obbCrossValidation, echo=FALSE, warning=FALSE, message=FALSE, fig.height = 9, fig.width = 9}
# As we are using each draw of the posterior distribution to predict the validation data, the dataset becames too big to store all species in memory.
# So I will loop over each species to (i) load dataset, (ii) predict for validation, (iii) compute prediction metrics and (iv) generate the plots

# add species_id to db
trainData[
  treeData,
  `:=`(
    'species_id' = i.species_id,
    'dbh' = i.dbh
  ),
  on = c('tree_id', 'year_measured')
]

# calculate `time` from year_measured
trainData[,
  time := year_measured - year_measured[which.min(year_measured)],
  by = tree_id
]


# list to save MSE and R2 for each species to be used later
MSE_list <- R2_list <- list()

for(Sp in spIds)
{
  val_sp <- trainData[species_id == Sp & sampled == 'validation']

  # replicate each observation by N_iteration to accommodate the parameters draw
  # Using only 1k iterations of the 8000
  val_sp[, obsID := .I]
  val_sp <- val_sp[rep(obsID, each = 1000)]
  sampledIter <- sample(1:(simInfo$maxIter/2 * simInfo$nC), 1000)
  val_sp[, iter := sampledIter, by = obsID]

  # merge global parameters
  val_sp <- merge(
      val_sp,
      post |>
          filter(species_id == Sp) |>
          pivot_wider(names_from = par, values_from = value) |>
          as.data.table(),
      by = c('iter')
  )
  
  # random iter were selected, create a new iter vetor to a proprer sequence
  # from 1 to nSample of iteration
  val_sp[, iter2 := 1:.N, by = obsID]

  # predict size
  val_sp[,
    pred_size := mu_Lo *
            exp(-r * time) +
            Lmax * (1 -exp(-r * time))
  ]

  val_sp[,
    sigma := sqrt((exp(-2 * r * time) * sigma_Lo^2) + sigma_obs^2)
  ]

  # simulate size from model distribution (normal dist)
  val_sp[,
      pred_size_random := rnorm(.N, pred_size, sigma)
  ]


  # calculate observed and predicted growth 
  val_sp[, time0 := shift(time, type = 'lag'), by = tree_id]
  val_sp[, obs_size0 := shift(dbh, type = 'lag'), by = tree_id]

  val_sp[, pred_size0 := shift(pred_size, type = 'lag'), by = tree_id]
  val_sp[, pred_size_random0 := shift(pred_size_random, type = 'lag'), by = tree_id]

  val_sp[, obs_growth := (dbh - obs_size0)/(time - time0)]
  val_sp[, pred_growth := (pred_size - pred_size0)/(time - time0)]
  val_sp[, pred_growth_random := (pred_size_random - pred_size_random0)/(time - time0)]

  # calculate MSE
  val_sp[, SE_size := (dbh - pred_size_random)^2]
  val_sp[, SE_growth := (obs_growth - pred_growth_random)^2]

  # mean SE over all observations for each iteration and save to list
  MSE_list[[Sp]] <- val_sp[, 
    .(
      MSE_size = mean(SE_size, na.rm = TRUE),
      MSE_growth = mean(SE_growth, na.rm = TRUE),
      species_id = Sp
    ),
    by = iter2
  ]

  # Calculate R2 (Gelman et al 2018)
  R2_list[[Sp]] <- val_sp |>
    group_by(iter2) |>
    summarise(var_size = var(pred_growth, na.rm = TRUE)) |>
    bind_cols(iter = sampledIter) |>
    left_join(
      post |>
        filter(species_id == Sp & par == 'sigma_obs') |>
        mutate(sigma_obs = value) |>
        select(iter, sigma_obs, species_id)
    ) |>
    mutate(r2 = var_size/(var_size + sigma_obs))

  # save MSE and R2 to be used for model comparison
  map_dfr(
    c('MSE', 'R2'),
    ~ get(paste0(.x, '_list')) |>
        bind_rows() |>
        saveRDS(file = file.path(
          'output',
          simName,
          paste0(.x, '_', Sp, '.RDS')
      )
    )
  )

  # plot
  p1 <- val_sp[time > 0, .(m_obs = mean(dbh), m_pred = mean(pred_size)), by = obsID] |>
      ggplot(aes(m_obs, m_pred)) +
          geom_hex(bins = 100) +
          geom_abline(slope = 1, intercept = 0) + 
          tune::coord_obs_pred() +
          theme(
                legend.position="none",
                plot.margin = margin(t = 30)
            ) +
          xlab('Observerd size') +
          ylab('Predicted size')

  p2 <- val_sp[time > 0, .(m_obs = mean(obs_growth), m_pred = mean(pred_growth)), by = obsID]  |>
      ggplot(aes(m_obs, m_pred)) +
          geom_hex(bins = 100) +
          geom_abline(slope = 1, intercept = 0) +
          xlim(-3, 20) + ylim(-3, 20) +
          theme(
                legend.position="none",
                plot.margin = margin(t = 30)
            ) +
          xlab('Observerd growth') +
          ylab('Predicted growth')

  pred_size_matrix <- val_sp[time > 0 & !is.na(pred_size_random), .(pred_size_random, iter2, obsID)] |>
      pivot_wider(names_from = obsID, values_from = pred_size_random) |>
      select(-iter2) |>
      as.matrix()

  p3 <- ppc_dens_overlay(
      val_sp[iter2 == 1 & time > 0, dbh],
      pred_size_matrix
  )

  pred_growth_matrix <- val_sp[time > 0 & !is.na(pred_growth), .(pred_growth_random, iter2, obsID)] |>
      pivot_wider(names_from = obsID, values_from = pred_growth_random) |>
      select(-iter2) |>
      as.matrix()

  p4 <- ppc_dens_overlay(
      val_sp[iter2 == 1 & time > 0, obs_growth],
      pred_growth_matrix
  )

  print(
    ggpubr::ggarrange(
        p1, p2, p3, p4,
        ncol = 2, nrow = 2,
        labels = Sp,
        font.label = list(size = 9, face = 'bold')
    )
  )
}
```


# Mean squared error (MSE) of out-of-bag predictions

```{r plot_MSE, echo = FALSE, warning=FALSE, message=FALSE, fig.height = 7, fig.width = 8}
MSE_list |>
  bind_rows() |>
  mutate(
    `Predicted size` = MSE_size,
    `Predicted growth` = MSE_growth
  ) |>
  select(-contains('MSE')) |>
  pivot_longer(
    cols = contains('Predicted')
  ) |>
  mutate(name2 = factor(name,  levels = c('Predicted size', 'Predicted growth'))) |>
  ggplot(aes(x = value, y = species_id)) +
    geom_density_ridges(alpha = 0.4, scale= 1.8) +
    facet_wrap(~name2, scales = 'free_x') +
    xlab('Mean Squared error (MSE)') + ylab('')
```


# Rsquared

The distribution of R squared values is calculated using the Gelman et al. [2018](http://www.stat.columbia.edu/~gelman/research/unpublished/bayes_R2.pdf) definition.

```{r plot Rsquared,echo=FALSE,fig.height = 6, fig.width = 7, warning=FALSE, message=FALSE}
R2_list |>
  bind_rows() |>
  ggplot(aes(x = r2, y = species_id)) +
    geom_density_ridges(alpha = 0.4, scale= 1.8)
```


# Sampling time

```{r plot sampling time, echo = FALSE, fig.height = 7, fig.width = 8}
map2_dfr(
  diag_files,
  names(diag_files),
  ~ readRDS(.x)[['time']][['chains']] |>
    bind_cols(species_id = .y)
) |>
ggplot(aes(x = total/3600, y = species_id)) +
  geom_boxplot() +
  xlab('Fit time (hours)')
```